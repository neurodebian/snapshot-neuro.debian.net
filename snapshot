#!/usr/bin/ruby

# Copyright (c) 2009 Peter Palfrader
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.


require 'optparse'
require 'yaml'
require 'dbi'
require 'logger'
require 'digest/sha1'

def barf(str)
	STDERR.puts str
	exit 1
end

class SnapshotDB
	def initialize(conf)
		@dbh = DBI.connect("dbi:Pg:#{conf['database']}:#{conf['host']}", conf['user'], conf['password'], 'AutoCommit'=>false)
	end

	def get_primarykey_name(table);
		# XXX
		return table+'_id';
	end


	def begin()
		@dbh.do("BEGIN")
	end
	def commit()
		@dbh.do("COMMIT")
	end

	def dbdo(query, *args)
		begin
			@dbh.do(query, *args)
		rescue DBI::ProgrammingError
			STDERR.puts("DB Error: #{$!}")
			STDERR.puts("Query: #{query}")
			STDERR.puts("Arguments: #{args.join(', ')}")
			raise
		end
	end
	def execute(query, *args)
		begin
			@dbh.execute(query, *args)
		rescue DBI::ProgrammingError
			STDERR.puts("DB Error: #{$!}")
			STDERR.puts("Query: #{query}")
			STDERR.puts("Arguments: #{args.join(', ')}")
			raise
		end
	end

	def insert(table, values, returning=nil)
		cols = values.keys().join(',')
		vals = values.values()
		qmarks = (['?'] * values.length).join(',')

		query = "INSERT INTO #{table} (#{cols}) VALUES (#{qmarks})"

		if returning.nil?
			dbdo(query, *vals)
			return nil
		else
			returning = [returning] unless returning.kind_of?(Array)
			query += " RETURNING #{returning.join(',')}"
			return query_row(query, *vals)
		end
	end

	def insert_row(table, values)
		pk_name = get_primarykey_name(table)
		if values.has_key?(pk_name)
			insert(table, values)
		else
			results = insert(table, values, [pk_name])
			values[pk_name] = results[pk_name]
		end
		values
	end

	def update(table, set, where, returning=nil)
		setclause = set.each_key.collect { |k| "#{k}=?" }.join(",")
		whereclause = where.each_key.collect { |k| where[k].nil? ? "#{k} IS NULL" : "#{k}=?" }.join(" AND ")

		query = "UPDATE #{table} SET #{setclause} WHERE #{whereclause}"

		params = set.values + where.values.compact
		unless returning
			return dbdo(query, *params)
		else
			returning = [returning] unless returning.kind_of?(Array)
			query += " RETURNING #{returning.join(',')}"
			return query_row(query, *params)
		end
	end

	def update_one(table, set, where)
		r = update(table, set, where)

		raise "Did not update exactly one row in update_one" unless r==1
		return r
	end

	def query(query, *params)
		sth = execute(query, *params)
		while row = sth.fetch_hash
			yield row
		end
		sth.finish
	end

	def query_row(query, *params)
		sth = execute(query, *params)

		row = sth.fetch_hash
		if row == nil
			sth.finish
			return nil
		elsif sth.fetch_hash != nil
			sth.finish
			raise "More than one result when querying for #{query}."
		else
			sth.finish
			return row
		end
	end
end

class FSNode
	def initialize(type, path, parent)
		@data = {}
		@type = type
		@path = _fixup_path(path)
		@name = File.basename(@path)
		@parent = parent
		if @parent.nil?
			raise "No parent but we are not /." unless @path == "/"
			@parent = self
		elsif not @parent.kind_of? FSNodeDirectory
			raise "FSNode got a parent of wrong type (#{parent.class})."
		end
	end

	def _fixup_path(path)
		return '/' if path == '.'
		return path[1..-1] if path[0..0] == '.'
		raise "Unexpected path(#{path}) for fixup"
	end

	def to_s
		"#{@type} #{@path}"
	end

	def new_db_node(db, mirrorrun_id)
		node = { 'first'  => mirrorrun_id,
		         'last'   => mirrorrun_id
		       }
		node['parent'] = @parent.directory_id(db)
		db.insert_row('node', node)
		return node['node_id']
	end


	def update_common(db, query, args)
		if self.kind_of? FSNodeDirectory
			query += ' RETURNING directory_id'
			r = db.query_row(query, *args)
			@directory_id = r['directory_id'] if r
			return !r.nil?
		else
			r = db.dbdo(query, *args)
			raise "Did not update exactly zero or one element." if r and r>1
			return r == 1
		end
	end

	def update_one_side(db, mirrorrun_id, lastnext, lastnext_id, table, whereclause, whereparams, isrootdir)
		query = "UPDATE node SET #{lastnext}=?
		         FROM #{table}
		         WHERE node.node_id = #{table}.node_id " +
		         (isrootdir ? "" : "AND parent=? ") +
		         "AND #{lastnext}=?
		         AND #{whereclause}"
		args = [mirrorrun_id] +
		       (isrootdir ? [] : [@parent.directory_id()]) +
		       [lastnext_id] +
		       whereparams
		return update_common(db, query, args)
	end

	def update_boxed(db, table, whereclause, whereparams, isrootdir)
		# if we have a mirrorun in the past and one in the future
		# then this element might already be covered by a node that starts
		# in the past and ends in the future.
		query = "DELETE FROM nodes_window USING #{table}
		         WHERE nodes_window.node_id = #{table}.node_id " +
		         (isrootdir ? "" : "AND parent=? ") +
		         "AND #{whereclause}"
		args = (isrootdir ? [] : [@parent.directory_id()]) + whereparams

		return update_common(db, query, args)
	end

	def insert(db, mirrorrun_id)
		new_node_id = new_db_node(db, mirrorrun_id)
		insert_elem(db, new_node_id)
	end

	def update_or_insert(db, mirrorrun_id, prev_run, next_run)
		table, whereclause, whereparams = already_exists_args
		isrootdir = @parent == self

		[ ['last', prev_run], ['first', next_run] ].each do |lastnext, lastnext_id|
			next if lastnext_id.nil?
			r = update_one_side(db, mirrorrun_id, lastnext, lastnext_id, table, whereclause, whereparams, isrootdir)
			return if r
		end

		r = update_boxed(db, table, whereclause, whereparams, isrootdir)
		return if r

		insert(db, mirrorrun_id)
	end
end

class FSNodeDirectory < FSNode
	def initialize(path, parent)
		super('d', path, parent)
	end

	def directory_id(db=nil)
		return @directory_id if @directory_id
		raise "This should only be required for the / directory.  This is #{@path}." unless @path == "/"
		@directory_id = db.query_row("SELECT nextval(pg_get_serial_sequence('directory', 'directory_id')) AS newref")['newref']
		$logger.debug("Making up directory_id for #{@path}.  It's #{@directory_id}.")
		return @directory_id
	end

	def already_exists_args()
		return 'directory', "path=?", [@path]
	end

	def insert_elem(db, node_id)
		dir = { 'path'    => @path,
		        'node_id' => node_id
		      }
		dir['directory_id'] = @directory_id if @directory_id
		db.insert_row('directory', dir)
		@directory_id = dir['directory_id']
	end
end

class FSNodeSymlink < FSNode
	def initialize(path, target, parent)
		super('l', path, parent)
		@target = target
	end
	def to_s
		super + " #{@target}"
	end

	def already_exists_args()
		return 'symlink', "name=? AND target=?", [@name, @target]
	end

	def insert_elem(db, node_id)
		elem = { 'name'    => @name,
		         'target'  => @target,
		         'node_id' => node_id
		       }
		db.insert_row('symlink', elem)
	end
end

class FSNodeRegular < FSNode
	def initialize(path, truepath, statinfo, parent, quick=nil)
		# a word on the quick parameter:
		#  quick is either a boolean or a time object.
		#  if it's false (or nil) then all files are always hashed
		#  and their digest checked to see if the entry exists in the DB.
		#
		#  same if quick is a timestamp and the file has ctime
		#  and mtime both older than quick.
		#
		#  else the quick path is taken and we only use size
		#  (in addition to the filename, and path) to see if
		#  we already are in the DB.  we only hash the file
		#  when we actually need to put that information in the DB,
		#  i.e. when the file is new.

		super('-', path, parent)
		@truepath = truepath
		@size = statinfo.size
		@time = statinfo.mtime

		most_recent_touched = [statinfo.mtime, statinfo.ctime].max
		if quick
			if quick.kind_of? Time
				return if most_recent_touched < quick
			else
				return
			end
		end
		@digest = Digest::SHA1.hexdigest( File.open(@truepath).read )
	end

	def to_s
		@digest.nil? ?
			(super + " #{@size} #{@time}") :
			(super + " #{@size} #{@time} #{@digest}")
	end

	def already_exists_args()
		if @digest.nil?
			return 'file', "name=? AND size=?", [@name, @size]
		else
			return 'file', "name=? AND size=? AND hash=?", [@name, @size, @digest]
		end
	end

	def insert_elem(db, node_id)
		@digest = Digest::SHA1.hexdigest( File.open(@truepath).read ) unless @digest
		elem = { 'name'    => @name,
		         'size'    => @size,
		         'hash'    => @digest,
		         'node_id' => node_id
		       }
		db.insert_row('file', elem)
	end
end

class FSReader
	def initialize(path, quick=nil)
		@root = path
		@quick = quick
	end

	def each_node(path='.', parent=nil)
		realpath = "#{@root}/#{path}"
		dir = FSNodeDirectory.new(path, parent)
		yield dir

		Dir.foreach(realpath) do |filename|
			next if %w{. ..}.include? filename
			element = "#{path}/#{filename}"
			trueelement = "#{@root}/#{element}"

			statinfo = File.lstat(trueelement)
			if statinfo.symlink?
				yield FSNodeSymlink.new(element, File.readlink(trueelement), dir)
			elsif statinfo.directory?
				each_node(element, dir) { |e| yield e}
			elsif statinfo.file?
				yield FSNodeRegular.new(element, trueelement, statinfo, dir, @quick)
			else
				$logger.warn("Ignoring #{element} which has unknown file type.")
			end
		end
	end
end

class SnapshotImporter
	def initialize(db, archive, path, date, quick)
		@db = db
		@archive = archive
		@path = path
		@date = date.nil? ? Time.new() : Time.parse(date)
		@quick = quick
	end

	# if we insert a new mirrorrun in time between existing mirrorruns
	# then it can happen that a specific file/dir/symlink existed in the
	# past and exists in the future, but does not currently exist.
	# in such cases we have to split existing nodes in the DB in two.
	def split_boxed_missing(mirrorrun_id, prev_run, next_run)
		# This code path is probably slightly tested at best
		# It will not get much practice

		handled = 0
		changed_parents = {}

		[ ['directory', %w(path) ],
		  ['symlink' , %w(name target)],
		  ['file', %w(name size hash)]
		].each do |type, items|
			query = "SELECT #{type}.#{type}_id, "+(items.collect{|i| "#{type}.#{i}, "}.join(""))+
			        "node.node_id, node.parent, node.first, node.last
					 FROM nodes_window JOIN node ON nodes_window.node_id = node.node_id
					                   JOIN #{type} ON node.node_id = #{type}.node_id"

			query += " ORDER BY path" if type == "directory"

			@db.query(query) do |row|
				@db.update_one('node', {'last' => prev_run}, {'node_id' => row['node_id']})

				# we would need to handle parent == self crap
				throw "Cannot split the / node." if type == 'directory' and row['path'] == "/"

				$logger.debug "Splitting #{row.inspect}"
				new_parent = (changed_parents[row['parent']] or row['parent'])
				new_node = {'parent' => new_parent, 'first' => next_run, 'last' => row['last']}
				@db.insert_row('node', new_node)

				new_elem = {}
				items.each { |k| new_elem[k] = row[k] }
				new_elem['node_id'] = new_node['node_id']
				@db.insert_row(type, new_elem)

				changed_parents[ row['directory_id'] ] = new_elem['directory_id'] if type == "directory"

				handled += 1
			end
		end

		row = @db.query_row("SELECT count(*) AS count FROM nodes_window")
		raise "Did not process correct amount of elements in nodes_window table." if row['count'] != handled
	end

	def import
		@db.begin

		row = @db.query_row('SELECT archive_id FROM archive WHERE name=?', @archive)
		barf("Archive #{archive} does not exist") if row.nil?
		archive_id = row['archive_id']

		mirrorrun_id = @db.insert_row('mirrorrun', {'archive_id'=>archive_id, 'run'=>@date})['mirrorrun_id']

		row = @db.query_row("SELECT
		                       (SELECT count(*) FROM mirrorrun WHERE archive_id=?
		                        AND run=(SELECT run FROM mirrorrun WHERE mirrorrun_id=?)) AS count,
		                       (SELECT mirrorrun_id FROM mirrorrun WHERE archive_id=?
		                        AND run>(SELECT run FROM mirrorrun WHERE mirrorrun_id=?)
		                        ORDER BY run
		                        LIMIT 1) AS next,
		                       (SELECT mirrorrun_id FROM mirrorrun WHERE archive_id=?
		                        AND run<(SELECT run FROM mirrorrun WHERE mirrorrun_id=?)
		                        ORDER BY run DESC
		                        LIMIT 1) AS prev
		                    ", archive_id, mirrorrun_id, archive_id, mirrorrun_id, archive_id, mirrorrun_id);
		barf("Cannot have two runs for the same archive at the exact same time.") if row['count'] > 1
		next_run, prev_run = row['next'], row['prev']

		@db.dbdo('CREATE TEMPORARY TABLE nodes_window AS
		           SELECT node_id, parent FROM node_with_ts
		           WHERE archive_id = ?
		             AND first_run < (SELECT run FROM mirrorrun WHERE mirrorrun_id=?)
		             AND last_run  > (SELECT run FROM mirrorrun WHERE mirrorrun_id=?)',
		         archive_id, mirrorrun_id, mirrorrun_id)

		barf("Quick imports are probably not safe when we have imports from the future already in the DB.") if next_run and @quick
		$logger.info("New mirrorrun #{mirrorrun_id} for #{@archive}")

		quick_cutoff_time = nil
		if @quick and prev_run
			row = @db.query_row("SELECT run FROM mirrorrun WHERE mirrorrun_id=?", prev_run)
			quick_cutoff_time=row['run']
		end

		fs = FSReader.new(@path, quick_cutoff_time)
		fs.each_node do |fsnode|
			fsnode.update_or_insert(@db, mirrorrun_id, prev_run, next_run)
			$logger.debug("Importing #{fsnode}")
		end

		split_boxed_missing(mirrorrun_id, prev_run, next_run)

		@db.dbdo("DROP TABLE nodes_window")
		@db.commit
	end

end

class Snapshot
	def initialize(conf)
		@db = SnapshotDB.new(conf['db'])
	end

	def list_archives()
		@db.query("SELECT name FROM archive ORDER BY name") { |row|
			puts row['name']
		}
	end

	def add_archive(options)
		barf("No archive name given") unless options['archive']
		@db.begin
		@db.insert_row('archive', {'name' => options['archive']})
		@db.commit
		$logger.info("Added new archive #{options['archive']}.")
	end

	def fs_list(options)
		barf("No path name given") unless options['path']
		fs = FSReader.new(options['path'], true)
		fs.each_node() { |p| puts p }
	end

	def import(options)
		barf("No path name given") unless options['path']
		barf("No archive name given") unless options['archive']
		fs = SnapshotImporter.new(@db, options['archive'], options['path'], options['date'], options['quick'])
		fs.import
	end
end


def makeLogger(conf, options)
	if options['log-to-stderr']
		logger = Logger.new(STDOUT)
	else
		barf("Configuration does not have a log->filename.") unless conf and conf['filename']
		logger = Logger.new(conf['filename'], shift_age = 'weekly')
	end

	if options['quiet']
		logger.level = Logger::WARN
	elsif options['verbose']
		logger.level = Logger::DEBUG
	else
		logger.level = Logger::INFO
	end

	logger.datetime_format = "%Y-%m-%d %H:%M:%S"

	return logger
end

def loadConfig(file)
	begin
		config = YAML::load( File.open( file ) )
	rescue
		barf("Cannot load config: #{$!}.")
	end
end


def show_help(parser, code=0, io=STDOUT)
	program_name = File.basename($0, '.*')
	io.puts "Usage: #{program_name} [options] <action>"
	io.puts "  Where action is one of the following:"
	io.puts "    list-archives                    List all known archives"
	io.puts "    add-archive                      Add a new archive"
	io.puts "    fs-list                          List a directory recursively"
	io.puts "    import                           Import a directory"
	io.puts "  Options:"
	io.puts parser.summarize
	exit(code)
end

options = {}
ARGV.options do |opts|
	opts.on_tail("-c", "--config=<f>"   , String, "Config file")                 { |options['config']| }
	opts.on_tail("-s", "--log-to-stderr", nil,    "Log to stderr instead of the configure logfile") { |options['log-to-stderr']| }
	opts.on_tail("-q", "--quiet"        , nil,    "Do not show info level notices")                 { |options['quiet']| }
	opts.on_tail("-v", "--verbose"      , nil,    "Be more verbose")                                { |options['verbose']| }
	opts.on_tail("-a", "--archive=<a>"  , String, "Archive to add/operate on")                      { |options['archive']| }
	opts.on_tail("-p", "--path=<p>"     , String, "Path to recurse/import")                         { |options['path']| }
	opts.on_tail("-d", "--date=<d>"     , String, "Date associated with an import (if not 'now')")  { |options['date']| }
	opts.on_tail("-Q", "--quick"        , nil,    "Quick import (use [cm]time and filesize, not hash)")  { |options['quick']| }

	opts.on_tail("-h", "--help", "Display this help screen")        { show_help(opts) }
	opts.parse!
end




show_help(ARGV.options, 1, STDERR) unless options['config']
show_help(ARGV.options, 1, STDERR) unless ARGV.length == 1
action = ARGV.shift

config = loadConfig(options['config'])
$logger = makeLogger(config['log'], options)
snapshot = Snapshot.new(config)

case action
	when "list-archives"
		snapshot.list_archives
	when "add-archive"
		snapshot.add_archive(options)
	when "fs-list"
		snapshot.fs_list(options)
	when "import"
		snapshot.import(options)
	else
		barf("Unknown action #{action}.")
end

# vim:set ts=4:
# vim:set shiftwidth=4:
