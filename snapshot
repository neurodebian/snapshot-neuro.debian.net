#!/usr/bin/ruby

# Copyright (c) 2009 Peter Palfrader
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.


require 'optparse'
require 'yaml'
require 'dbi'
require 'logger'
require 'digest/sha1'
require 'digest/md5'
require 'ftools'

def barf(str)
	$logger.error(str)
	exit 1
end


class StorageBackend
	def store(source, digest)
		throw "Not implemented"
	end
	def open(digest)
		throw "Not implemented"
	end
end

class FileBackend < StorageBackend
	def initialize(farmpath)
		@farmpath = farmpath
	end

	def _get_path(digest, mkdir=false)
		target = @farmpath
		h = digest

		2.times do
		  target = target + '/' + h[0..1]
		  h = h[2..-1]
		  Dir.mkdir(target) if mkdir and not File.directory?(target)
		end

		target = target + '/' + digest

	end

	def store(source, digest)
		target = _get_path(digest, true)
		File.copy(source, target) unless File.exists?(target)
	end

	def open(digest)
		target = _get_path(digest)
		return File.new(target)
	end

	def get_path_to(digest)
		return _get_path(digest)
	end
end

class SnapshotDB
	def initialize(conf, logger)
		@dbh = DBI.connect("dbi:Pg:#{conf['database']}:#{conf['host']}", conf['user'], conf['password'], 'AutoCommit'=>false)
		@logger = logger
	end

	def get_primarykey_name(table);
		# XXX
		return table+'_id';
	end


	def begin()
		@dbh.do("BEGIN")
	end
	def commit()
		@dbh.do("COMMIT")
	end

	def dbdo(query, *args)
		begin
			@dbh.do(query, *args)
		rescue DBI::ProgrammingError
			@logger.warn("DB Error: #{$!}")
			@logger.warn("Query: #{query}")
			@logger.warn("Arguments: #{args.join(', ')}")
			raise
		end
	end
	def execute(query, *args)
		begin
			@dbh.execute(query, *args)
		rescue DBI::ProgrammingError
			@logger.warn("DB Error: #{$!}")
			@logger.warn("Query: #{query}")
			@logger.warn("Arguments: #{args.join(', ')}")
			raise
		end
	end

	def insert(table, values, returning=nil)
		cols = values.keys().join(',')
		vals = values.values()
		qmarks = (['?'] * values.length).join(',')

		query = "INSERT INTO #{table} (#{cols}) VALUES (#{qmarks})"

		if returning.nil?
			dbdo(query, *vals)
			return nil
		else
			returning = [returning] unless returning.kind_of?(Array)
			query += " RETURNING #{returning.join(',')}"
			return query_row(query, *vals)
		end
	end

	def insert_row(table, values)
		pk_name = get_primarykey_name(table)
		if values.has_key?(pk_name)
			insert(table, values)
		else
			results = insert(table, values, [pk_name])
			values[pk_name] = results[pk_name]
		end
		values
	end

	def update(table, set, where, returning=nil)
		setclause = set.each_key.collect { |k| "#{k}=?" }.join(",")
		whereclause = where.each_key.collect { |k| where[k].nil? ? "#{k} IS NULL" : "#{k}=?" }.join(" AND ")

		query = "UPDATE #{table} SET #{setclause} WHERE #{whereclause}"

		params = set.values + where.values.compact
		unless returning
			return dbdo(query, *params)
		else
			returning = [returning] unless returning.kind_of?(Array)
			query += " RETURNING #{returning.join(',')}"
			return query_row(query, *params)
		end
	end

	def update_one(table, set, where)
		r = update(table, set, where)

		raise "Did not update exactly one row in update_one" unless r==1
		return r
	end

	def query(query, *params)
		sth = execute(query, *params)
		while row = sth.fetch_hash
			yield row
		end
		sth.finish
	end

	def query_row(query, *params)
		sth = execute(query, *params)

		row = sth.fetch_hash
		if row == nil
			sth.finish
			return nil
		elsif sth.fetch_hash != nil
			sth.finish
			raise "More than one result when querying for #{query}."
		else
			sth.finish
			return row
		end
	end
end

class FSNode
	def initialize(type, path, parent)
		@data = {}
		@type = type
		@path = _fixup_path(path)
		@name = File.basename(@path)
		@parent = parent
		if @parent.nil?
			raise "No parent but we are not /." unless @path == "/"
			@parent = self
		elsif not @parent.kind_of? FSNodeDirectory
			raise "FSNode got a parent of wrong type (#{parent.class})."
		end
	end

	def _fixup_path(path)
		return '/' if path == '.'
		return path[1..-1] if path[0..0] == '.'
		raise "Unexpected path(#{path}) for fixup"
	end

	def to_s
		"#{@type} #{@path}"
	end

	def new_db_node(db, mirrorrun_id)
		node = { 'first'  => mirrorrun_id,
		         'last'   => mirrorrun_id
		       }
		node['parent'] = @parent.directory_id(db)
		db.insert_row('node', node)
		return node['node_id']
	end


	def update_common(db, query, args)
		if self.kind_of? FSNodeDirectory
			query += ' RETURNING directory_id'
			r = db.query_row(query, *args)
			@directory_id = r['directory_id'] if r
			return !r.nil?
		else
			r = db.dbdo(query, *args)
			raise "Did not update exactly zero or one element." if r and r>1
			return r == 1
		end
	end

	def update_one_side(db, mirrorrun_id, lastnext, lastnext_id, table, whereclause, whereparams, isrootdir)
		query = "UPDATE node SET #{lastnext}=?
		         FROM #{table}
		         WHERE node.node_id = #{table}.node_id " +
		         (isrootdir ? "" : "AND parent=? ") +
		         "AND #{lastnext}=?
		         AND #{whereclause}"
		args = [mirrorrun_id] +
		       (isrootdir ? [] : [@parent.directory_id()]) +
		       [lastnext_id] +
		       whereparams
		return update_common(db, query, args)
	end

	def update_boxed(db, table, whereclause, whereparams, isrootdir)
		# if we have a mirrorun in the past and one in the future
		# then this element might already be covered by a node that starts
		# in the past and ends in the future.
		query = "DELETE FROM nodes_window USING #{table}
		         WHERE nodes_window.node_id = #{table}.node_id " +
		         (isrootdir ? "" : "AND parent=? ") +
		         "AND #{whereclause}"
		args = (isrootdir ? [] : [@parent.directory_id()]) + whereparams

		return update_common(db, query, args)
	end

	def insert(db, mirrorrun_id)
		new_node_id = new_db_node(db, mirrorrun_id)
		insert_elem(db, new_node_id)
	end

	def update_or_insert(db, mirrorrun_id, prev_run, next_run)
		table, whereclause, whereparams = already_exists_args
		isrootdir = @parent == self

		[ ['last', prev_run], ['first', next_run] ].each do |lastnext, lastnext_id|
			next if lastnext_id.nil?
			r = update_one_side(db, mirrorrun_id, lastnext, lastnext_id, table, whereclause, whereparams, isrootdir)
			return if r
		end

		r = update_boxed(db, table, whereclause, whereparams, isrootdir)
		return if r

		insert(db, mirrorrun_id)
	end
end

class FSNodeDirectory < FSNode
	def initialize(path, parent)
		super('d', path, parent)
	end

	def directory_id(db=nil)
		return @directory_id if @directory_id
		raise "This should only be required for the / directory.  This is #{@path}." unless @path == "/"
		@directory_id = db.query_row("SELECT nextval(pg_get_serial_sequence('directory', 'directory_id')) AS newref")['newref']
		$logger.debug("Making up directory_id for #{@path}.  It's #{@directory_id}.")
		return @directory_id
	end

	def already_exists_args()
		return 'directory', "path=?", [@path]
	end

	def insert_elem(db, node_id)
		dir = { 'path'    => @path,
		        'node_id' => node_id
		      }
		dir['directory_id'] = @directory_id if @directory_id
		db.insert_row('directory', dir)
		@directory_id = dir['directory_id']
	end
end

class FSNodeSymlink < FSNode
	def initialize(path, target, parent)
		super('l', path, parent)
		@target = target
	end
	def to_s
		super + " #{@target}"
	end

	def already_exists_args()
		return 'symlink', "name=? AND target=?", [@name, @target]
	end

	def insert_elem(db, node_id)
		elem = { 'name'    => @name,
		         'target'  => @target,
		         'node_id' => node_id
		       }
		db.insert('symlink', elem)
	end
end

class FSNodeRegular < FSNode
	def initialize(path, truepath, statinfo, parent, quick=nil)
		# a word on the quick parameter:
		#  quick is either a boolean or a time object.
		#  if it's false (or nil) then all files are always hashed
		#  and their digest checked to see if the entry exists in the DB.
		#
		#  same if quick is a timestamp and the file has ctime
		#  and mtime both older than quick.
		#
		#  else the quick path is taken and we only use size
		#  (in addition to the filename, and path) to see if
		#  we already are in the DB.  we only hash the file
		#  when we actually need to put that information in the DB,
		#  i.e. when the file is new.

		super('-', path, parent)
		@truepath = truepath
		@size = statinfo.size
		@time = statinfo.mtime

		most_recent_touched = [statinfo.mtime, statinfo.ctime].max
		if quick
			if quick.kind_of? Time
				return if most_recent_touched < quick
			else
				return
			end
		end
		@digest = Digest::SHA1.hexdigest( File.open(@truepath).read )
	end

	def to_s
		@digest.nil? ?
			(super + " #{@size} #{@time}") :
			(super + " #{@size} #{@time} #{@digest}")
	end

	def already_exists_args()
		if @digest.nil?
			return 'file', "name=? AND size=?", [@name, @size]
		else
			return 'file', "name=? AND size=? AND hash=?", [@name, @size, @digest]
		end
	end

	def insert_elem(db, node_id)
		@digest = Digest::SHA1.hexdigest( File.open(@truepath).read ) unless @digest
		elem = { 'name'    => @name,
		         'size'    => @size,
		         'hash'    => @digest,
		         'node_id' => node_id
		       }
		db.insert('file', elem)

		$storage.store(@truepath, @digest)
	end
end

class FSReader
	def initialize(path, quick=nil)
		@root = path
		@quick = quick
	end

	def each_node(path='.', parent=nil)
		realpath = "#{@root}/#{path}"
		dir = FSNodeDirectory.new(path, parent)
		yield dir

		Dir.foreach(realpath) do |filename|
			next if %w{. ..}.include? filename
			element = "#{path}/#{filename}"
			trueelement = "#{@root}/#{element}"

			statinfo = File.lstat(trueelement)
			if statinfo.symlink?
				yield FSNodeSymlink.new(element, File.readlink(trueelement), dir)
			elsif statinfo.directory?
				each_node(element, dir) { |e| yield e}
			elsif statinfo.file?
				yield FSNodeRegular.new(element, trueelement, statinfo, dir, @quick)
			else
				$logger.warn("Ignoring #{element} which has unknown file type.")
			end
		end
	end
end

class SnapshotImporter
	def initialize(db, archive, path, date, quick, timetravel)
		@db = db
		@archive = archive
		@path = path
		@date = date.nil? ? Time.new() : Time.parse(date)
		@quick = quick
		@timetravel = timetravel
	end

	# if we insert a new mirrorrun in time between existing mirrorruns
	# then it can happen that a specific file/dir/symlink existed in the
	# past and exists in the future, but does not currently exist.
	# in such cases we have to split existing nodes in the DB in two.
	def split_boxed_missing(mirrorrun_id, prev_run, next_run)
		# This code path is probably slightly tested at best
		# It will not get much practice

		handled = 0
		changed_parents = {}

		[ ['directory', %w(path) ],
		  ['symlink' , %w(name target)],
		  ['file', %w(name size hash)]
		].each do |type, items|
			query = "SELECT #{type}.#{type}_id, "+(items.collect{|i| "#{type}.#{i}, "}.join(""))+
			        "node.node_id, node.parent, node.first, node.last
					 FROM nodes_window JOIN node ON nodes_window.node_id = node.node_id
					                   JOIN #{type} ON node.node_id = #{type}.node_id"

			query += " ORDER BY path" if type == "directory"

			@db.query(query) do |row|
				@db.update_one('node', {'last' => prev_run}, {'node_id' => row['node_id']})

				# we would need to handle parent == self crap
				throw "Cannot split the / node." if type == 'directory' and row['path'] == "/"

				$logger.debug "Splitting #{row.inspect}"
				new_parent = (changed_parents[row['parent']] or row['parent'])
				new_node = {'parent' => new_parent, 'first' => next_run, 'last' => row['last']}
				@db.insert_row('node', new_node)

				new_elem = {}
				items.each { |k| new_elem[k] = row[k] }
				new_elem['node_id'] = new_node['node_id']
				@db.insert_row(type, new_elem)

				changed_parents[ row['directory_id'] ] = new_elem['directory_id'] if type == "directory"

				handled += 1
			end
		end

		row = @db.query_row("SELECT count(*) AS count FROM nodes_window")
		raise "Did not process correct amount of elements in nodes_window table." if row['count'] != handled
	end

	def import
		@db.begin

		row = @db.query_row('SELECT archive_id FROM archive WHERE name=?', @archive)
		barf("Archive #{archive} does not exist") if row.nil?
		archive_id = row['archive_id']

		mirrorrun_id = @db.insert_row('mirrorrun', {'archive_id'=>archive_id, 'run'=>@date})['mirrorrun_id']

		row = @db.query_row("SELECT
		                       (SELECT count(*) FROM mirrorrun WHERE archive_id=?
		                        AND run=(SELECT run FROM mirrorrun WHERE mirrorrun_id=?)) AS count,
		                       (SELECT mirrorrun_id FROM mirrorrun WHERE archive_id=?
		                        AND run>(SELECT run FROM mirrorrun WHERE mirrorrun_id=?)
		                        ORDER BY run
		                        LIMIT 1) AS next,
		                       (SELECT mirrorrun_id FROM mirrorrun WHERE archive_id=?
		                        AND run<(SELECT run FROM mirrorrun WHERE mirrorrun_id=?)
		                        ORDER BY run DESC
		                        LIMIT 1) AS prev
		                    ", archive_id, mirrorrun_id, archive_id, mirrorrun_id, archive_id, mirrorrun_id);
		barf("Cannot have two runs for the same archive at the exact same time.") if row['count'] > 1
		next_run, prev_run = row['next'], row['prev']

		@db.dbdo('CREATE TEMPORARY TABLE nodes_window AS
		           SELECT node_id, parent FROM node_with_ts
		           WHERE archive_id = ?
		             AND first_run < (SELECT run FROM mirrorrun WHERE mirrorrun_id=?)
		             AND last_run  > (SELECT run FROM mirrorrun WHERE mirrorrun_id=?)',
		         archive_id, mirrorrun_id, mirrorrun_id)

		barf("Quick imports are probably not safe when we have imports from the future already in the DB.") if next_run and @quick
		barf("Enable --timetravel if you want to import archives older than the newest in the DB.") if next_run and not @timetravel
		$logger.info("New mirrorrun #{mirrorrun_id} for #{@archive}")

		quick_cutoff_time = nil
		if @quick and prev_run
			row = @db.query_row("SELECT run FROM mirrorrun WHERE mirrorrun_id=?", prev_run)
			quick_cutoff_time=row['run']
		end

		fs = FSReader.new(@path, quick_cutoff_time)
		fs.each_node do |fsnode|
			fsnode.update_or_insert(@db, mirrorrun_id, prev_run, next_run)
			$logger.debug("Importing #{fsnode}")
		end

		split_boxed_missing(mirrorrun_id, prev_run, next_run)

		@db.dbdo("DROP TABLE nodes_window")
		@db.commit
	end
end

class PackageIndexer
	def initialize(db, quick)
		@db = db
		@quick = quick

		@sourcepkgs = {}
		@binarypkgs = {}
	end

	def get_file_digest(mirrorrun_id, filename, quick=false)
		row = quick ?
			@db.query_row("SELECT get_file_from_mirrorrun_with_path_first(?, ?, ?) AS hash", mirrorrun_id, File.dirname(filename), File.basename(filename)) :
			@db.query_row("SELECT get_file_from_mirrorrun_with_path(?, ?, ?) AS hash", mirrorrun_id, File.dirname(filename), File.basename(filename))
		return row['hash']
	end

	def open_file(mirrorrun_id, filename, quick=false)
		digest = get_file_digest(mirrorrun_id, filename, quick)
		return nil if digest.nil?
		return $storage.open(digest)
	end

	def add_pkg(type, pkg, ver, srcpkg_id=nil)
		case type
			when "src"
				cache = @sourcepkgs
				throw "don't need a srcpkg_id" unless srcpkg_id.nil?
			when "bin"
				cache = @binarypkgs
				throw "need a srcpkg_id" if srcpkg_id.nil?
			else
				throw "Invalud type #{type}"
		end

		return cache[pkg][ver] if cache[pkg] and cache[pkg][ver]

		query = "SELECT #{type}pkg_id AS id FROM #{type}pkg WHERE name=? AND version=?"
		args = [pkg, ver]
		if srcpkg_id
			query += " AND srcpkg_id=?"
			args << srcpkg_id
		end
		
		r = @db.query_row(query, *args)
		if r
			id = r["id"]
		else
			p = {'name' => pkg, 'version' => ver }
			p['srcpkg_id'] = srcpkg_id if srcpkg_id
			@db.insert_row("#{type}pkg", p)
			id = p["#{type}pkg_id"]
		end

		cache[pkg] = {} unless cache[pkg]
		cache[pkg][ver] = id
		return id
	end

	def add_srcpkg(pkg, ver)
		add_pkg('src', pkg, ver)
	end
	def add_binpkg(pkg, ver, srcpkg_id)
		add_pkg('bin', pkg, ver, srcpkg_id)
	end

	def hash_has_any_key(hash, keys)
		keys.each{ |k| return true if hash.has_key? k }
		return false
	end
	def hash_has_all_keys(hash, keys)
		keys.each{ |k| return false unless hash.has_key? k }
		return true
	end

	def insert_file_from_digest(type, pkg, digest, arch = nil)
		query = "SELECT count(*) AS cnt FROM file_#{type}pkg_mapping WHERE #{type}pkg_id=? AND hash=?"
		args = [pkg, digest]
		if arch
			query += " AND architecture=?"
			args << arch
		end

		r = @db.query_row(query, *args)
		return if r['cnt'] == 1
		throw "Unexpected count of #{r['cnt']}" unless r['cnt'] == 0

		p = {"#{type}pkg_id" => pkg, 'hash' => digest}
		p['architecture'] = arch if arch
		@db.insert("file_#{type}pkg_mapping", p)
	end

	def insert_file_from_path(type, pkg, mirrorrun_id, path, arch = nil)
		path = path.clone
		path[0..0] = '' if path[0..0] = '.'
		case type
			when "src"
				throw "don't need an arch" unless arch.nil?
			when "bin"
				throw "need an arch" if arch.nil?
			else
				throw "Invalud type #{type}"
		end

		digest = get_file_digest(mirrorrun_id, path, @quick)
		unless digest
			$logger.warn("File #{path} is referenced in index but does not exist in mirrorrun #{mirrorrun_id})") unless @quick
			return
		end

		insert_file_from_digest(type, pkg, digest, arch)
	end

	def insert_src_file_from_path(srcpkg, mirrorrun_id, path)
		insert_file_from_path('src', srcpkg, mirrorrun_id, path)
	end
	def insert_bin_file_from_path(binpkg, mirrorrun_id, path, arch)
		insert_file_from_path('bin', binpkg, mirrorrun_id, path, arch)
	end
	def insert_src_file_from_digest(pkg, digest)
		insert_file_from_digest('src', pkg, digest)
	end
	def insert_bin_file_from_digest(pkg, digest, arch)
		insert_file_from_digest('bin', pkg, digest, arch)
	end


	# Insert binary and source packages listed in /indices/package-file.map.bz2
	# if the mirror has such a file.
	def index_mirrorrun_from_index(mirrorrun_id)
		index = open_file(mirrorrun_id, '/indices/package-file.map.bz2')
		return unless index
		index = BZ2::Reader.new(index)
		lineno = 0
		index.each_line(sep_string='') do |block|
			e = {}
			block.split("\n").each do |line|
				key,value = line.split(/: */, 2)
				e[key] = value
				lineno += 1
			end
			lineno += 1

			unless hash_has_all_keys(e, %w(Path))
				$logger.warn("Block has no path element before line #{lineno}")
				next
			end

			unless hash_has_all_keys(e, %w(Source Source-Version))
				$logger.warn("Block has incomplete source information before line #{lineno}")
				next
			end

			srcpkg = add_srcpkg(e['Source'], e['Source-Version'])
			if not hash_has_any_key(e, %w(Binary-Version Binary Architecture))
				$logger.debug("Inserting #{e['Path']} for source #{e['Source']} #{e['Source-Version']}")
				insert_src_file_from_path(srcpkg, mirrorrun_id, e['Path'])
			else
				unless hash_has_all_keys(e, %w(Binary-Version Binary Architecture))
					$logger.warn("Block has incomplete binary information before line #{lineno}")
					next
				end

				binpkg = add_binpkg(e['Binary'], e['Binary-Version'], srcpkg)
				$logger.debug("Inserting #{e['Path']} for binary #{e['Binary']} #{e['Binary-Version']}")
				insert_bin_file_from_path(binpkg, mirrorrun_id, e['Path'], e['Architecture'])
			end
		end
		source = "index"
		source += '(Q)' if @quick
		return source
	end

	# index a given .deb or .udeb.
	# package, version, source etc are all learned from dpkg --info.
	def index_binary_package(mirrorrun_id, path, name, digest)
		e = {}
		fd = IO.popen('-') do |fd|
			if not fd
				cmd = ['dpkg', '--info', $storage.get_path_to(digest)]
				exec(*cmd)
				$logger.error("Failed to exec #{cmd.join(" ")}")
				exit(1)
			end

			fd.each_line do |line|
				line.chomp!
				next if line =~ /^  /
				line[0..0] = '' if line[0..0] == ' '

				key,value = line.split(/: */, 2)
				next unless value
				e[key] = value
			end
		end

		unless hash_has_all_keys(e, %w(Version Package Architecture))
			$logger.warn("dpkg --info returned incomplete info for #{name} (#{digest})")
			return
		end
		e['Binary'] = e['Package']
		e['Binary-Version'] = e['Version']

		if e.has_key?('Source')
			s = e['Source']
			e['Source'], e['Source-Version'] = s.split(' ')
			if e['Source-Version']
				m = /^\((.*)\)$/.match(e['Source-Version'])
				unless m
					$logger.warn("Did not get source version out of #{s} for #{name} (#{digest})")
					return
				end
				e['Source-Version'] = m[1]
			else
				e['Source-Version'] = e['Version']
			end
		else
			e['Source'] = e['Package']
			e['Source-Version'] = e['Version']
		end

		srcpkg = add_srcpkg(e['Source'], e['Source-Version'])
		binpkg = add_binpkg(e['Binary'], e['Binary-Version'], srcpkg)
		insert_bin_file_from_digest(binpkg, digest, e['Architecture'])
		$logger.debug("Inserting for binary #{e['Binary']} #{e['Binary-Version']}: #{name} (#{digest})")
	end

	# index the source package given by the dscdigest.
	# imports the dsc, and any files referenced in it into the database.
	# if the .dsc lists sha1 sums we use them directly,
	# if it only lists md5 sums we search the file by name, check the
	# md5 sum matches and import it into the db.
	def index_source_package(mirrorrun_id, path, name, dscdigest)
		fd = $storage.open(dscdigest)
		lastkey = nil
		e = {}
		# skip pgp header
		fd.each_line do |line|
			break if line =~ /^Format: /
		end
		fd.each_line do |line|
			line.chomp!
			if line[0..0] == ' ' and lastkey
				e[lastkey] = [] unless e[lastkey]
				e[lastkey] << line[1..-1]
			else
				key, value = line.split(/: */)
				e[key] = value
				lastkey = key
			end
			break if line == "" # skip pgp footer
		end
		fd.close

		unless hash_has_all_keys(e, %w(Source Version))
			$logger.warn("dsc file #{name} has incomplete source information")
			return
		end

		e['Source-Version'] = e['Version']
		srcpkg = add_srcpkg(e['Source'], e['Source-Version'])
		insert_src_file_from_digest(srcpkg, dscdigest)
		$logger.debug("Inserting for source #{e['Source']} #{e['Source-Version']}: #{name} (#{dscdigest})")

		if e['Checksums-Sha1']
			e['Checksums-Sha1'].each do |line|
				hash, size, itemname, rest = line.split(/ /)
				insert_src_file_from_digest(srcpkg, hash)
				$logger.debug("Inserting for source #{e['Source']} #{e['Source-Version']}: #{itemname} (#{hash})")
			end
		else
			e['Files'].each do |line|
				expect_md5, size, itemname, rest = line.split(/ /)
				sha1 = get_file_digest(mirrorrun_id, path+"/"+itemname)
				unless sha1
					$logger.warn("dsc file #{name} references #{itemname} which we do not have")
					next
				end

				is_md5 = Digest::MD5.hexdigest( $storage.open(sha1).read )
				if is_md5 != expect_md5
					$logger.warn("dsc file #{name} references #{itemname} (#{expect_md5}) but we have md5 hash #{is_md5}")
					next
				end

				insert_src_file_from_digest(srcpkg, sha1)
				$logger.debug("Inserting for source #{e['Source']} #{e['Source-Version']}: #{itemname} (#{sha1})")
			end
		end
	end

	# Recurse into a path, indexing any binary and source packages found.
	def recurse_into(mirrorrun_id, path)
		if not @quick
			query = "SELECT filetype, name, digest FROM readdir(?,?)"
			args = [path, mirrorrun_id]
		else
			query = "SELECT filetype, name, digest FROM readdir(?,?) AS readdir JOIN node ON node.node_id = readdir.node_id WHERE (NOT filetype = '-') OR node.first=?"
			args = [path, mirrorrun_id, mirrorrun_id]
		end
		@db.query(query, *args) do |row|
			case row['filetype']
				when "d"
					recurse_into(mirrorrun_id, path+"/"+row['name'])
				when "-"
					case row['name']
						when /\.deb$|\.udeb/
							index_binary_package(mirrorrun_id, path, row['name'], row['digest'])
						when /\.dsc/
							index_source_package(mirrorrun_id, path, row['name'], row['digest'])
					end
			end
		end
	end

	# If there is no /indices/package-file.map.bz2 we have to fall back to recursing over the tree
	def index_mirrorrun_from_parsing(mirrorrun_id)
		recurse_into(mirrorrun_id, '/pool')
		source = 'recurse'
		source += '(Q)' if @quick
		return source
	end

	def index_mirrorrun(mirrorrun_id)
		source = index_mirrorrun_from_index(mirrorrun_id)
		return source if source
		source = index_mirrorrun_from_parsing(mirrorrun_id)
		return source
	end

	def index
		@db.query("SELECT mirrorrun_id, run, (SELECT name FROM archive WHERE archive.archive_id = mirrorrun.archive_id) AS archive
		           FROM mirrorrun WHERE NOT mirrorrun_id IN (SELECT mirrorrun_id FROM indexed_mirrorrun) ORDER BY run") do |row|
			@db.begin
			$logger.info("Indexing mirrorrun ##{row['mirrorrun_id']} of #{row['archive']} from #{row['run']}")
			source = index_mirrorrun(row['mirrorrun_id'])
			@db.insert('indexed_mirrorrun', {' mirrorrun_id' => row['mirrorrun_id'], 'source' => source })
			@db.commit
		end
	end
end


class Snapshot
	def initialize(conf)
		@db = SnapshotDB.new(conf['db'], $logger)
	end

	def list_archives()
		@db.query("SELECT name FROM archive ORDER BY name") { |row|
			puts row['name']
		}
	end

	def add_archive(options)
		barf("No archive name given") unless options['archive']
		@db.begin
		@db.insert('archive', {'name' => options['archive']})
		@db.commit
		$logger.info("Added new archive #{options['archive']}.")
	end

	def fs_list(options)
		barf("No path name given") unless options['path']
		fs = FSReader.new(options['path'], true)
		fs.each_node() { |p| puts p }
	end

	def import(options)
		barf("No path name given") unless options['path']
		barf("No archive name given") unless options['archive']
		fs = SnapshotImporter.new(@db, options['archive'], options['path'], options['date'], options['quick'], options['timetravel'])
		fs.import
	end

	def index(options)
		pi = PackageIndexer.new(@db, options['quick'])
		pi.index
	end
end

def makeLogger(conf, options)
	if options['log-to-stderr']
		logger = Logger.new(STDOUT)
	else
		barf("Configuration does not have a log->filename.") unless conf and conf['filename']
		logger = Logger.new(conf['filename'], shift_age = 'weekly')
	end

	if options['quiet']
		logger.level = Logger::WARN
	elsif options['verbose']
		logger.level = Logger::DEBUG
	else
		logger.level = Logger::INFO
	end

	logger.datetime_format = "%Y-%m-%d %H:%M:%S"

	return logger
end

def loadConfig(file)
	begin
		config = YAML::load( File.open( file ) )
	rescue
		barf("Cannot load config: #{$!}.")
	end
end


def show_help(parser, code=0, io=STDOUT)
	program_name = File.basename($0, '.*')
	io.puts "Usage: #{program_name} [options] <action>"
	io.puts "  Where action is one of the following:"
	io.puts "    list-archives                    List all known archives"
	io.puts "    add-archive                      Add a new archive"
	io.puts "    fs-list                          List a directory recursively"
	io.puts "    import                           Import a directory"
	io.puts "    index                            Index packages in archives"
	io.puts "  Options:"
	io.puts parser.summarize
	exit(code)
end

options = {}
ARGV.options do |opts|
	opts.on_tail("-c", "--config=<f>"   , String, "Config file")                 { |options['config']| }
	opts.on_tail("-s", "--log-to-stderr", nil,    "Log to stderr instead of the configure logfile") { |options['log-to-stderr']| }
	opts.on_tail("-q", "--quiet"        , nil,    "Do not show info level notices")                 { |options['quiet']| }
	opts.on_tail("-v", "--verbose"      , nil,    "Be more verbose")                                { |options['verbose']| }
	opts.on_tail("-a", "--archive=<a>"  , String, "Archive to add/operate on")                      { |options['archive']| }
	opts.on_tail("-p", "--path=<p>"     , String, "Path to recurse/import")                         { |options['path']| }
	opts.on_tail("-d", "--date=<d>"     , String, "Date associated with an import (if not 'now')")  { |options['date']| }
	opts.on_tail("-Q", "--quick"        , nil,    "Quick import (use [cm]time and filesize, not hash), Quick index (only index files that were added in a mirrorrun)")  { |options['quick']| }
	opts.on_tail("-T", "--timetravel"   , nil,    "Allow imports at dates that are not later than all existing mirrorruns")  { |options['timetravel']| }

	opts.on_tail("-h", "--help", "Display this help screen")        { show_help(opts) }
	opts.parse!
end




show_help(ARGV.options, 1, STDERR) unless options['config']
show_help(ARGV.options, 1, STDERR) unless ARGV.length == 1
action = ARGV.shift

config = loadConfig(options['config'])
$logger = makeLogger(config['log'], options)

barf("Configuration does not have a snapshot->farmpath.") unless config and config['snapshot'] and config['snapshot']['farmpath']
$storage = FileBackend.new(config['snapshot']['farmpath'])
snapshot = Snapshot.new(config)

case action
	when "list-archives"
		snapshot.list_archives
	when "add-archive"
		snapshot.add_archive(options)
	when "fs-list"
		snapshot.fs_list(options)
	when "import"
		snapshot.import(options)
	when 'index'
		require 'bz2'
		snapshot.index(options)
	else
		barf("Unknown action #{action}.")
end

# vim:set ts=4:
# vim:set shiftwidth=4:
