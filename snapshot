#!/usr/bin/ruby

# Copyright (c) 2009, 2010 Peter Palfrader
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.


require 'optparse'
require 'yaml'
require 'dbi'
require 'logger'
require 'digest/sha1'
require 'digest/md5'
require 'fileutils'
require 'time'

def barf(str)
	if $logger.nil?
		STDERR.puts(str)
		STDERR.puts("(Also, while trying to print this we realized there $logger is not defined!)")
		exit 1
	end
	$logger.error(str)
	exit 1
end

def randstring()
	value = ''
	8.times{value  << (65 + rand(25)).chr}
	value
end


class StorageBackend
	def store(source, digest)
		throw "Not implemented"
	end
	def open(digest)
		throw "Not implemented"
	end
end

class FileBackend < StorageBackend
	def initialize(farmpath, db)
		@farmpath = farmpath
		@db = db
	end

	def add_to_journal(hash)
		@db.insert_row('farm_journal', { 'hash'  => hash } )
	end

	def _get_path(digest, mkdir=false)
		target = @farmpath
		h = digest

		2.times do
		  target = target + '/' + h[0..1]
		  h = h[2..-1]
		  Dir.mkdir(target) if mkdir and not File.directory?(target)
		end

		target = target + '/' + digest
	end

	def exists?(digest)
		target = _get_path(digest)
		return File.exists?(target)
	end

	def store(source, digest)
		unless exists?(digest)
			target = _get_path(digest, true)
			dir = File.dirname(target)
			fn = File.basename(target)

			tmptarget = dir+"/.tmp."+randstring()+"."+(Process.pid.to_s)+"."+fn
			FileUtils.cp(source, tmptarget)
			begin
				File.link(tmptarget, target)
				add_to_journal(digest)
			rescue Errno::EEXIST
				# it may have jumped into existance, that's no problem.
			end
			File.unlink(tmptarget)
		end
	end

	def open(digest)
		target = _get_path(digest)
		return File.new(target)
	end

	def get_path_to(digest)
		return _get_path(digest)
	end
end

class SnapshotDB
	def initialize(conf, logger)
		s = []
		s << "database=#{conf['database']}"
		s << "host=#{conf['host']}" if conf['host']
		s << "port=#{conf['port']}" if conf['port']

		@dbh = DBI.connect("dbi:Pg:#{s.join(';')}", conf['user'], conf['password'], 'AutoCommit'=>false)
		@logger = logger
	end

	def get_primarykey_name(table);
		# XXX
		return table+'_id';
	end


	def begin()
		@dbh.do("BEGIN")
	end
	def commit()
		@dbh.do("COMMIT")
	end

	def dbdo(query, *args)
		begin
			@dbh.do(query, *args)
		rescue DBI::ProgrammingError
			@logger.warn("DB Error: #{$!}")
			@logger.warn("Query: #{query}")
			@logger.warn("Arguments: #{args.join(', ')}")
			raise
		end
	end
	def execute(query, *args)
		begin
			@dbh.execute(query, *args)
		rescue DBI::ProgrammingError
			@logger.warn("DB Error: #{$!}")
			@logger.warn("Query: #{query}")
			@logger.warn("Arguments: #{args.join(', ')}")
			raise
		end
	end

	def insert(table, values, returning=nil)
		cols = values.keys().join(',')
		vals = values.values()
		qmarks = (['?'] * values.length).join(',')

		query = "INSERT INTO #{table} (#{cols}) VALUES (#{qmarks})"

		if returning.nil?
			dbdo(query, *vals)
			return nil
		else
			returning = [returning] unless returning.kind_of?(Array)
			query += " RETURNING #{returning.join(',')}"
			return query_row(query, *vals)
		end
	end

	def insert_row(table, values)
		pk_name = get_primarykey_name(table)
		if values.has_key?(pk_name)
			insert(table, values)
		else
			results = insert(table, values, [pk_name])
			values[pk_name] = results[pk_name]
		end
		values
	end

	def update(table, set, where, returning=nil)
		setclause = set.each_key.collect { |k| "#{k}=?" }.join(",")
		whereclause = where.each_key.collect { |k| where[k].nil? ? "#{k} IS NULL" : "#{k}=?" }.join(" AND ")

		query = "UPDATE #{table} SET #{setclause} WHERE #{whereclause}"

		params = set.values + where.values.compact
		unless returning
			return dbdo(query, *params)
		else
			returning = [returning] unless returning.kind_of?(Array)
			query += " RETURNING #{returning.join(',')}"
			return query_row(query, *params)
		end
	end

	def update_one(table, set, where)
		r = update(table, set, where)

		raise "Did not update exactly one row in update_one" unless r==1
		return r
	end

	def query(query, *params)
		sth = execute(query, *params)
		while row = sth.fetch_hash
			yield row
		end
		sth.finish
	end

	def query_row(query, *params)
		sth = execute(query, *params)

		row = sth.fetch_hash
		if row == nil
			sth.finish
			return nil
		elsif sth.fetch_hash != nil
			sth.finish
			raise "More than one result when querying for #{query}."
		else
			sth.finish
			return row
		end
	end
end

class FSNode
	attr_reader :path

	def initialize(type, path, parent)
		@data = {}
		@type = type
		@path = _fixup_path(path)
		@name = File.basename(@path)
		@parent = parent
		if @parent.nil?
			raise "No parent but we are not /." unless @path == "/"
			@parent = self
		elsif not @parent.kind_of? FSNodeDirectory
			raise "FSNode got a parent of wrong type (#{parent.class})."
		end
	end

	def _fixup_path(path)
		return '/' if path == '.'
		return path[1..-1] if path[0..0] == '.'
		raise "Unexpected path(#{path}) for fixup"
	end

	def to_s
		"#{@type} #{@path}"
	end

	def new_db_node(db, mirrorrun_id)
		node = { 'first'  => mirrorrun_id,
		         'last'   => mirrorrun_id
		       }
		node['parent'] = @parent.directory_id(db)
		db.insert_row('node', node)
		return node['node_id']
	end


	def update_common(db, query, args)
		if self.kind_of? FSNodeDirectory
			query += ' RETURNING directory_id'
			r = db.query_row(query, *args)
			@directory_id = r['directory_id'] if r
			return !r.nil?
		else
			r = db.dbdo(query, *args)
			raise "Did not update exactly zero or one element." if r and r>1
			return r == 1
		end
	end

	def update_one_side(db, mirrorrun_id, lastnext, lastnext_id, table, whereclause, whereparams, isrootdir)
		query = "UPDATE node SET #{lastnext}=?
		         FROM #{table}
		         WHERE node.node_id = #{table}.node_id " +
		         (isrootdir ? "" : "AND parent=? ") +
		         "AND #{lastnext}=?
		         AND #{whereclause}"
		args = [mirrorrun_id] +
		       (isrootdir ? [] : [@parent.directory_id()]) +
		       [lastnext_id] +
		       whereparams
		return update_common(db, query, args)
	end

	def update_boxed(db, table, whereclause, whereparams, isrootdir)
		# if we have a mirrorrun in the past and one in the future
		# then this element might already be covered by a node that starts
		# in the past and ends in the future.
		query = "DELETE FROM nodes_window USING #{table}
		         WHERE nodes_window.node_id = #{table}.node_id " +
		         (isrootdir ? "" : "AND parent=? ") +
		         "AND #{whereclause}"
		args = (isrootdir ? [] : [@parent.directory_id()]) + whereparams

		return update_common(db, query, args)
	end

	def insert(db, mirrorrun_id)
		new_node_id = new_db_node(db, mirrorrun_id)
		insert_elem(db, new_node_id)
	end

	def update_or_insert(db, mirrorrun_id, prev_run, next_run)
		table, whereclause, whereparams = already_exists_args
		isrootdir = @parent == self

		[ ['last', prev_run], ['first', next_run] ].each do |lastnext, lastnext_id|
			next if lastnext_id.nil?
			r = update_one_side(db, mirrorrun_id, lastnext, lastnext_id, table, whereclause, whereparams, isrootdir)
			return if r
		end

		r = update_boxed(db, table, whereclause, whereparams, isrootdir)
		return if r

		insert(db, mirrorrun_id)
	end
end

class FSNodeDirectory < FSNode
	def initialize(path, parent)
		super('d', path, parent)
	end

	def directory_id(db=nil)
		return @directory_id if @directory_id
		raise "This should only be required for the / directory.  This is #{@path}." unless @path == "/"
		@directory_id = db.query_row("SELECT nextval(pg_get_serial_sequence('directory', 'directory_id')) AS newref")['newref']
		$logger.debug("Making up directory_id for #{@path}.  It's #{@directory_id}.")
		return @directory_id
	end

	def already_exists_args()
		return 'directory', "path=?", [@path]
	end

	def insert_elem(db, node_id)
		dir = { 'path'    => @path,
		        'node_id' => node_id
		      }
		dir['directory_id'] = @directory_id if @directory_id
		db.insert_row('directory', dir)
		@directory_id = dir['directory_id']
	end
end

class FSNodeSymlink < FSNode
	def initialize(path, target, parent)
		super('l', path, parent)
		@target = target
	end
	def to_s
		super + " #{@target}"
	end

	def already_exists_args()
		return 'symlink', "name=? AND target=?", [@name, @target]
	end

	def insert_elem(db, node_id)
		elem = { 'name'    => @name,
		         'target'  => @target,
		         'node_id' => node_id
		       }
		db.insert('symlink', elem)
	end
end

class FSNodeRegularBase < FSNode
	def initialize(path, parent, size, digest)
		super('-', path, parent)
		@size = size
		@digest = digest
	end

	def to_s
		@digest.nil? ?
			(super + " #{@size}") :
			(super + " #{@size} #{@digest}")
	end

	def already_exists_args()
		if @digest.nil?
			return 'file', "name=? AND size=?", [@name, @size]
		else
			return 'file', "name=? AND size=? AND hash=?", [@name, @size, @digest]
		end
	end

	def get_digest
		return @digest if @digest

		# this should only happen if a child class doesn't override it and also doesn't set @digest.
		throw "get_digest() called in FSNodeRegularBase and we don't have an answer.  That's not good."
	end

	def store_file
		throw "store_file() called in FSNodeRegularBase - children should overwrite it."
	end

	def insert_elem(db, node_id)
		elem = { 'name'    => @name,
		         'size'    => @size,
		         'hash'    => get_digest,
		         'node_id' => node_id
		       }
		db.insert('file', elem)

		store_file
	end
end

class FSNodeRegular < FSNodeRegularBase
	def initialize(path, truepath, statinfo, parent, quick=nil)
		# a word on the quick parameter:
		#  quick is either a boolean or a time object.
		#  if it's false (or nil) then all files are always hashed
		#  and their digest checked to see if the entry exists in the DB.
		#
		#  same if quick is a timestamp and the file has ctime
		#  and mtime both older than quick.
		#
		#  else the quick path is taken and we only use size
		#  (in addition to the filename, and path) to see if
		#  we already are in the DB.  we only hash the file
		#  when we actually need to put that information in the DB,
		#  i.e. when the file is new.

		@truepath = truepath

		digest = nil
		get_digest = true
		most_recent_touched = [statinfo.mtime, statinfo.ctime].max
		if quick
			if quick.kind_of? Time
				get_digest = false if most_recent_touched < quick
			else
				get_digest = false
			end
		end
		if get_digest
			digest = Digest::SHA1.file(@truepath).hexdigest
		end

		super(path, parent, statinfo.size, digest)
	end

	def get_digest
		return @digest if @digest
		@digest = Digest::SHA1.file(@truepath).hexdigest
		@digest
	end

	def store_file
		$storage.store(@truepath, get_digest)
	end
end

class FSNodeRegularFromDump < FSNodeRegularBase
	def initialize(path, parent, size, digest, trust_files_are_there)
		super(path, parent, size, digest)
		@trust_files_are_there = trust_files_are_there
	end

	def store_file
		return if @trust_files_are_there
		return if $storage.exists?(@digest)
		$logger.warn("[dump import] missing file in storage: #{@digest} #{@path}.")
	end
end

class FSReader
	def initialize(path, quick=nil)
		@root = path
		@quick = quick
	end

	def each_node(path='.', parent=nil)
		realpath = "#{@root}/#{path}"
		dir = FSNodeDirectory.new(path, parent)
		yield dir

		Dir.foreach(realpath) do |filename|
			next if %w{. ..}.include? filename
			element = "#{path}/#{filename}"
			trueelement = "#{@root}/#{element}"

			statinfo = File.lstat(trueelement)
			if statinfo.symlink?
				yield FSNodeSymlink.new(element, File.readlink(trueelement), dir)
			elsif statinfo.directory?
				each_node(element, dir) { |e| yield e}
			elsif statinfo.file?
				yield FSNodeRegular.new(element, trueelement, statinfo, dir, @quick)
			else
				$logger.warn("Ignoring #{element} which has unknown file type.")
			end
		end
	end
end

class DumpReader
	def initialize(path, trust_files_are_there)
		@fd = File.open( path )
		@dir_cache = {}
		@attrs = {}
		@trust_files_are_there

		while not (line = @fd.gets).nil?
			line.chomp!
			key,value = line.split(/: */, 2)
			@attrs[key] = value
			break if key == "Contents"
		end

		%w{Archive Date UUID ImportingHost Contents}.each do |k|
			barf("Required key #{k} not found in dump") unless @attrs.has_key?(k)
		end
	end

	def archive
		@attrs['Archive']
	end
	def uuid
		@attrs['UUID']
	end
	def date
		@attrs['Date']
	end
	def importing_host
		@attrs['ImportingHost']
	end

	def each_node(path='.', parent=nil)
		line = @fd.gets
		barf "Reached end of file without any contents?" unless line
		line.chomp!
		barf "Expected 'd /' as first entry" unless line == " d /"

		dir = FSNodeDirectory.new(path, parent)
		@dir_cache[dir.path] = dir
		yield dir

		@fd.each_line do |line|
			line.chomp!
			barf "Invalid input line '#{line}'." unless line[0..0] == ' '
			line[0..0] = ''

			type,path,rest = line.split(' ', 3)
			barf "Expected path to start with /" unless path[0..0] == "/"
			parentpath = File.dirname(path)
			path = '.'+path

			parent = @dir_cache[parentpath]
			barf "Failed while trying to find parentdir #{parentpath} of entry '#{line}'" unless parent

			case type
				when 'd'
					dir = FSNodeDirectory.new(path, parent)
					@dir_cache[dir.path] = dir
					yield dir
				when '-'
					size, digest = rest.split(' ',2)
					throw "Invalid size #{size} in entry '#{line}'" if size =~ /[^0-9]/
					throw "Invalid digest #{digest} in entry '#{line}'" if digest =~ /[^0-9a-f]/
					throw "Invalid digest #{digest} in entry '#{line}'" if digest.length != 40
					yield FSNodeRegularFromDump.new(path, parent, size, digest, @trust_files_are_there)
				when 'l'
					yield FSNodeSymlink.new(path, rest, parent)
				else
					barf "Unknown type #{type} in line '#{line}'"
			end
		end
	end
end


class SnapshotImporter
	def initialize(db, timetravel)
		@db = db
		@timetravel = timetravel
	end

	# if we insert a new mirrorrun in time between existing mirrorruns
	# then it can happen that a specific file/dir/symlink existed in the
	# past and exists in the future, but does not currently exist.
	# in such cases we have to split existing nodes in the DB in two.
	def _split_boxed_missing(mirrorrun_id, prev_run, next_run)
		# This code path is probably slightly tested at best
		# It will not get much practice

		handled = 0
		changed_parents = {}

		[ ['directory', %w(path) ],
		  ['symlink' , %w(name target)],
		  ['file', %w(name size hash)]
		].each do |type, items|
			query = "SELECT #{type}.#{type}_id, "+(items.collect{|i| "#{type}.#{i}, "}.join(""))+
			        "node.node_id, node.parent, node.first, node.last
					 FROM nodes_window JOIN node ON nodes_window.node_id = node.node_id
					                   JOIN #{type} ON node.node_id = #{type}.node_id"

			query += " ORDER BY path" if type == "directory"

			@db.query(query) do |row|
				@db.update_one('node', {'last' => prev_run}, {'node_id' => row['node_id']})

				# we would need to handle parent == self crap
				throw "Cannot split the / node." if type == 'directory' and row['path'] == "/"

				$logger.debug "[run ##{mirrorrun_id}] Splitting #{row.inspect}"
				new_parent = (changed_parents[row['parent']] or row['parent'])
				new_node = {'parent' => new_parent, 'first' => next_run, 'last' => row['last']}
				@db.insert_row('node', new_node)

				new_elem = {}
				items.each { |k| new_elem[k] = row[k] }
				new_elem['node_id'] = new_node['node_id']
				@db.insert_row(type, new_elem)

				changed_parents[ row['directory_id'] ] = new_elem['directory_id'] if type == "directory"

				handled += 1
			end
		end

		row = @db.query_row("SELECT count(*) AS count FROM nodes_window")
		raise "Did not process correct amount of elements in nodes_window table." if row['count'].to_i != handled
	end


	def _get_archive_id(archive)
		row = @db.query_row('SELECT archive_id FROM archive WHERE name=? FOR UPDATE', archive)
		barf("Archive #{archive} does not exist") if row.nil?
		return row['archive_id']
	end

	def _insert_mirrorrun(archive_id, date, uuid=nil, importing_host=nil)
		date = date.nil? ? Time.new() : Time.parse(date)
		uuid=`uuidgen`.chomp if uuid.nil?
		importing_host=`hostname -f`.chomp if importing_host.nil?

		mirrorrun_id = @db.insert_row('mirrorrun', {'archive_id'=>archive_id, 'run'=>date.to_s, 'mirrorrun_uuid'=>uuid, 'importing_host'=>importing_host})['mirrorrun_id']
		return mirrorrun_id
	end

	def _get_prev_next(archive_id, mirrorrun_id)
		row = @db.query_row("SELECT
		                       (SELECT count(*) FROM mirrorrun WHERE archive_id=?
		                        AND run=(SELECT run FROM mirrorrun WHERE mirrorrun_id=?)) AS count,
		                       (SELECT mirrorrun_id FROM mirrorrun WHERE archive_id=?
		                        AND run>(SELECT run FROM mirrorrun WHERE mirrorrun_id=?)
		                        ORDER BY run
		                        LIMIT 1) AS next,
		                       (SELECT mirrorrun_id FROM mirrorrun WHERE archive_id=?
		                        AND run<(SELECT run FROM mirrorrun WHERE mirrorrun_id=?)
		                        ORDER BY run DESC
		                        LIMIT 1) AS prev
		                    ", archive_id, mirrorrun_id, archive_id, mirrorrun_id, archive_id, mirrorrun_id);
		barf("Cannot have two runs for the same archive at the exact same time.") if row['count'].to_i > 1

		return row['prev'], row['next']
	end

	def _create_nodes_window(archive_id, mirrorrun_id)
		@db.dbdo('CREATE TEMPORARY TABLE nodes_window AS
		           SELECT node_id, parent FROM node_with_ts
		           WHERE archive_id = ?
		             AND first_run < (SELECT run FROM mirrorrun WHERE mirrorrun_id=?)
		             AND last_run  > (SELECT run FROM mirrorrun WHERE mirrorrun_id=?)',
		         archive_id, mirrorrun_id, mirrorrun_id)
		@db.dbdo('CREATE INDEX nodes_window_idx_parent ON nodes_window(parent)')
		@db.dbdo('ANALYZE nodes_window')
	end
	def _cleanup_nodes_window()
		@db.dbdo("DROP TABLE nodes_window")
	end

	def _get_quick_cutoff_time(quick, prev_run)
		quick_cutoff_time = nil
		if quick and prev_run
			row = @db.query_row("SELECT run FROM mirrorrun WHERE mirrorrun_id=?", prev_run)
			quick_cutoff_time=Time.parse(row['run'].to_s)
		end
		return quick_cutoff_time
	end

	def _create_fs_reader(quick, prev_run, path)
		quick_cutoff_time = _get_quick_cutoff_time(quick, prev_run)
		fs = FSReader.new(path, quick_cutoff_time)
		return fs
	end

	def import_from_filesystem(path, archive, date, quick)
		@db.begin

		archive_id = _get_archive_id(archive)
		mirrorrun_id = _insert_mirrorrun(archive_id, date)
		prev_run, next_run = _get_prev_next(archive_id, mirrorrun_id)
		fs = _create_fs_reader(quick, prev_run, path)
		barf("Quick imports are probably not safe when we have imports from the future already in the DB.") if next_run and quick

		$logger.info("New mirrorrun #{mirrorrun_id} for #{archive}")
		import(archive_id, mirrorrun_id, fs, prev_run, next_run)
		@db.commit
		$logger.info("[run ##{mirrorrun_id}] Mirrorrun #{mirrorrun_id} for #{archive} completed.")
	end

	def import_from_dump(path, quick)
		@db.begin

		fs = DumpReader.new(path, quick)
		archive_id = _get_archive_id(fs.archive)
		mirrorrun_id = _insert_mirrorrun(archive_id, fs.date, fs.uuid, fs.importing_host)
		prev_run, next_run = _get_prev_next(archive_id, mirrorrun_id)

		$logger.info("Import mirrorrun #{mirrorrun_id} for #{fs.archive}")
		import(archive_id, mirrorrun_id, fs, prev_run, next_run)
		@db.commit
		$logger.info("[run ##{mirrorrun_id}] Mirrorrun #{mirrorrun_id} for #{fs.archive} completed (imported #{fs.uuid}).")
	end

	def import(archive_id, mirrorrun_id, fs, prev_run, next_run)
		barf("Enable --timetravel if you want to import archives older than the newest in the DB.") if next_run and not @timetravel

		_create_nodes_window(archive_id, mirrorrun_id)
		fs.each_node do |fsnode|
			fsnode.update_or_insert(@db, mirrorrun_id, prev_run, next_run)
			$logger.debug("[run ##{mirrorrun_id}] Importing #{fsnode}")
		end
		_split_boxed_missing(mirrorrun_id, prev_run, next_run)
		_cleanup_nodes_window()
	end
end

class PackageIndexer
	def initialize(db, quick, only_this_mirrorrun)
		@db = db
		@quick = quick
		@only_this_mirrorrun = only_this_mirrorrun

		@mirrorrun_id = nil
		@mirrorrun_run = nil
		@mirrorrun_archive_id = nil
	end

	def _get_archive_and_run_from_mirrorrun(mirrorrun_id)
		row = @db.query_row("SELECT run, archive_id FROM mirrorrun WHERE mirrorrun_id=?", mirrorrun_id)
		return row['archive_id'], row['run']
	end

	def get_file_digest(filename, mirrorrun_id=nil)
		if mirrorrun_id
			archive_id, run = _get_archive_and_run_from_mirrorrun(mirrorrun_id)
		else
			archive_id = @mirrorrun_archive_id
			run = @mirrorrun_run
		end
		row = @db.query_row("SELECT get_file_from_path_at(?, ?, ?, ?) AS hash", archive_id, run, File.dirname(filename), File.basename(filename))
		return row['hash']
	end

	def open_file(filename, mirrorrun_id=nil)
		digest = get_file_digest(filename, mirrorrun_id)
		return nil if digest.nil?
		return $storage.open(digest)
	end

	def add_pkg(type, pkg, ver, srcpkg_id=nil)
		case type
			when "src"
				cache = @sourcepkgs
				throw "don't need a srcpkg_id" unless srcpkg_id.nil?
			when "bin"
				cache = @binarypkgs
				throw "need a srcpkg_id" if srcpkg_id.nil?
			else
				throw "Invalud type #{type}"
		end

		return cache[pkg][ver] if cache[pkg] and cache[pkg][ver]

		query = "SELECT #{type}pkg_id AS id FROM #{type}pkg WHERE name=? AND version=?"
		args = [pkg, ver]
		if srcpkg_id
			query += " AND srcpkg_id=?"
			args << srcpkg_id
		end
		
		r = @db.query_row(query, *args)
		if r
			id = r["id"]
		else
			p = {'name' => pkg, 'version' => ver }
			p['srcpkg_id'] = srcpkg_id if srcpkg_id
			@db.insert_row("#{type}pkg", p)
			id = p["#{type}pkg_id"]
		end

		cache[pkg] = {} unless cache[pkg]
		cache[pkg][ver] = id
		return id
	end

	def add_srcpkg(pkg, ver)
		add_pkg('src', pkg, ver)
	end
	def add_binpkg(pkg, ver, srcpkg_id)
		add_pkg('bin', pkg, ver, srcpkg_id)
	end

	def hash_has_any_key(hash, keys)
		keys.each{ |k| return true if hash.has_key? k }
		return false
	end
	def hash_has_all_keys(hash, keys)
		keys.each{ |k| return false unless hash.has_key? k }
		return true
	end

	def insert_file_from_digest(type, pkg, digest, arch = nil)
		query = "SELECT count(*) AS cnt FROM file_#{type}pkg_mapping WHERE #{type}pkg_id=? AND hash=?"
		args = [pkg, digest]
		if arch
			query += " AND architecture=?"
			args << arch
		end

		r = @db.query_row(query, *args)
		r['cnt'] = r['cnt'].to_i
		return if r['cnt'] == 1
		throw "Unexpected count of #{r['cnt']}" unless r['cnt'] == 0

		p = {"#{type}pkg_id" => pkg, 'hash' => digest}
		p['architecture'] = arch if arch
		@db.insert("file_#{type}pkg_mapping", p)
	end

	def insert_file_from_path(type, pkg, path, arch = nil)
		case type
			when "src"
				throw "don't need an arch" unless arch.nil?
			when "bin"
				throw "need an arch" if arch.nil?
			else
				throw "Invalud type #{type}"
		end

		digest = get_file_digest(path)
		unless digest
			$logger.warn("[indexrun ##{@mirrorrun_id}] File #{path} is referenced in index but does not exist in mirrorrun)")
			return false
		end

		insert_file_from_digest(type, pkg, digest, arch)
		return true
	end

	def insert_src_file_from_path(srcpkg, path)
		insert_file_from_path('src', srcpkg, path)
	end
	def insert_bin_file_from_path(binpkg, path, arch)
		insert_file_from_path('bin', binpkg, path, arch)
	end
	def insert_src_file_from_digest(pkg, digest)
		insert_file_from_digest('src', pkg, digest)
	end
	def insert_bin_file_from_digest(pkg, digest, arch)
		insert_file_from_digest('bin', pkg, digest, arch)
	end


	# Insert binary and source packages listed in /indices/package-file.map.bz2
	# if the mirror has such a file.
	def index_mirrorrun_from_index()
		index = open_file('/indices/package-file.map.bz2')
		return unless index
		@sourcepkgs = {}
		@binarypkgs = {}

		previously_seen = nil
		if (@quick)
			row = @db.query_row("SELECT mirrorrun_id as prev FROM mirrorrun
			                       WHERE archive_id=?
			                         AND run < ?
			                       ORDER BY run DESC
			                       LIMIT 1", @mirrorrun_archive_id, @mirrorrun_run)
			unless row.nil?
				prev_run_id = row['prev']
				$logger.debug("[indexrun ##{@mirrorrun_id}] previous run was ##{prev_run_id}")
				prev_index = open_file('/indices/package-file.map.bz2', prev_run_id)
				unless prev_index.nil?
					previously_seen = {}
					begin
						prev_index = Bzip2::Reader.new(prev_index)
						prev_index.each_line(sep_string='') do |block|
							previously_seen[Digest::SHA1.digest(block)] = 1
						end
					rescue Bzip2::EOZError => e
						$logger.warn("[indexrun ##{@mirrorrun_id}] previous (##{prev_run_id}) package-file.map is corrupt (Bzip2::EOZError): #{e.message}")
						return
					end
				else
					$logger.warn("[indexrun ##{@mirrorrun_id}] quick mode selected but no previous (##{prev_run_id}) package-file.map")
				end
			end
		end

		@db.dbdo('SAVEPOINT startofindexing')
		begin
			lineno = 0
			index = Bzip2::Reader.new(index)
			index.each_line(sep_string='') do |block|
				next if previously_seen and previously_seen.has_key? Digest::SHA1.digest(block)

				e = {}
				block.split("\n").each do |line|
					key,value = line.split(/: */, 2)
					e[key] = value
					lineno += 1
				end
				lineno += 1

				unless hash_has_all_keys(e, %w(Path))
					$logger.warn("[indexrun ##{@mirrorrun_id}] Block has no path element before line #{lineno}")
					next
				end

				e['Path'][0..0] = '' if e['Path'][0..0] = '.'

				unless hash_has_all_keys(e, %w(Source Source-Version))
					$logger.warn("[indexrun ##{@mirrorrun_id}] Block has incomplete source information before line #{lineno}")
					next
				end

				srcpkg = add_srcpkg(e['Source'], e['Source-Version'])
				if not hash_has_any_key(e, %w(Binary-Version Binary Architecture))
					inserted = insert_src_file_from_path(srcpkg, e['Path'])
					$logger.debug("[indexrun ##{@mirrorrun_id}] " + (inserted ? "Inserting" : "Skipping already existing") + " #{e['Path']} for source #{e['Source']} #{e['Source-Version']}")
				else
					unless hash_has_all_keys(e, %w(Binary-Version Binary Architecture))
						$logger.warn("[indexrun ##{@mirrorrun_id}] Block has incomplete binary information before line #{lineno}")
						next
					end

					binpkg = add_binpkg(e['Binary'], e['Binary-Version'], srcpkg)
					inserted = insert_bin_file_from_path(binpkg, e['Path'], e['Architecture'])
					$logger.debug("[indexrun ##{@mirrorrun_id}] " + (inserted ? "Inserting" : "Skipping already existing") + " #{e['Path']} for binary #{e['Binary']} #{e['Binary-Version']}")
				end
			end
		rescue Bzip2::EOZError => e
			@db.dbdo('ROLLBACK TO startofindexing')
			$logger.warn("[indexrun ##{@mirrorrun_id}] package-file.map is corrupt (Bzip2::EOZError): #{e.message}")
			return
		end
		@db.dbdo('RELEASE SAVEPOINT startofindexing')
		source = "index"
		source += '(Q)' if @quick
		return source
	end

	# index a given .deb or .udeb.
	# package, version, source etc are all learned from dpkg --info.
	def index_binary_package(path, name, digest)
		e = {}
		debversion = nil
		fd = IO.popen('-') do |fd|
			if not fd
				cmd = ['dpkg', '--info', $storage.get_path_to(digest)]
				exec(*cmd)
				$logger.error("[indexrun ##{@mirrorrun_id}] Failed to exec #{cmd.join(" ")}")
				exit(1)
			end

			# first line
			line = fd.readline
			line.chomp!
			line[0..0] = '' if line[0..0] == ' '
			case line
				when "old debian package, version 0.932000.",
				     "old debian package, version 0.933000.",
				     "old debian package, version 0.936000.",
				     "old debian package, version 0.939000."
					debversion = '0.93'
				when "new debian package, version 2.0."
					debversion = '2'
				else
					throw "Unknown binary package version #{line}"
			end

			fd.each_line do |line|
				line.chomp!
				next if line =~ /^  /
				line[0..0] = '' if line[0..0] == ' '

				key,value = line.split(/: */, 2)
				next unless value
				# Some packages have fields, but their value is empty.
				# I'm looking at you, aout-xpm_3.4f-1.deb
				next if value == ''
				e[key.capitalize] = value
			end
		end

		throw "Why is debversion nil?" unless debversion

		if debversion == '0.93'
			e['Version'] += "-" + e['Package_revision'] if e['Package_revision']
		end
		e['Architecture'] = 'i386' unless e['Architecture']

		unless hash_has_all_keys(e, %w(Version Package Architecture))
			$logger.warn("[indexrun ##{@mirrorrun_id}] dpkg --info returned incomplete info for #{name} (#{digest}).  [existing keys: #{e.keys.join(', ')}]")
			return
		end
		e['Binary'] = e['Package']
		e['Binary-Version'] = e['Version']

		if e.has_key?('Source')
			s = e['Source']
			e['Source'], e['Source-Version'] = s.split(' ')
			if e['Source-Version']
				m = /^\((.*)\)$/.match(e['Source-Version'])
				if m
					e['Source-Version'] = m[1]
				else
					# Some packages do not have source-version in parenthenses:
					# SOURCE: ncurses 1.9.9e-1   in ncurses-term_1.9.9e-1.deb
					if e['Source-Version'] != ""
						$logger.warn("[indexrun ##{@mirrorrun_id}] Using '#{e['Source-Version']}' from '#{s}' as source version for #{path}/#{name} (#{digest})")
					else
						$logger.warn("[indexrun ##{@mirrorrun_id}] Did not get source version out of #{s} for #{path}/#{name} (#{digest})")
						return
					end
				end
			else
				e['Source-Version'] = e['Version']
			end
		else
			e['Source'] = e['Package']
			e['Source-Version'] = e['Version']
		end

		srcpkg = add_srcpkg(e['Source'], e['Source-Version'])
		binpkg = add_binpkg(e['Binary'], e['Binary-Version'], srcpkg)
		insert_bin_file_from_digest(binpkg, digest, e['Architecture'])
		$logger.debug("[indexrun ##{@mirrorrun_id}] Inserting for binary #{path}/#{name} (#{digest}: #{e['Binary']} #{e['Binary-Version']}")


		# .93 debs did not have .dsc files for their source.  Still try to find them
		if debversion == '0.93'
				pathelements = path.split('/')
				pathelements << name
				pathelements[-3] = 'source' if %w{binary binary-i386 binary-all}.include?(pathelements[-3])
				pathelements[-1] = File.basename(pathelements[-1], ".deb")

				a = []
				a.push( pathelements.join('/') + '.diff.gz' )
				a.push( pathelements.join('/') + '.tar.gz' )

				a.each do |f|
					sha1 = get_file_digest(f)
					unless sha1 # not found.  pitty
						$logger.debug("[indexrun ##{@mirrorrun_id}] No source found for #{e['Source']} #{e['Source-Version']}: #{f} (#{sha1})")
						next
					end

					insert_src_file_from_digest(srcpkg, sha1)
					$logger.debug("[indexrun ##{@mirrorrun_id}] Inserting for source #{f} #{digest}: #{e['Source']} #{e['Source-Version']}")
				end
		end
	end

	# index the source package given by the dscdigest.
	# imports the dsc, and any files referenced in it into the database.
	# if the .dsc lists sha1 sums we use them directly,
	# if it only lists md5 sums we search the file by name, check the
	# md5 sum matches and import it into the db.
	def index_source_package(path, name, dscdigest)
		# first line
		fd = $storage.open(dscdigest)
		lines = fd.readlines.collect{ |x| x.chomp }
		fd.close

		# skip pgp header
		if lines.first == '-----BEGIN PGP SIGNED MESSAGE-----'
			while lines.length > 0
				first = lines.shift
				break if first == ""
			end
		end

		lastkey = nil
		e = {}
		lines.each do |line|
			break if line == "" # skip pgp footer
			break if line == "-----BEGIN PGP SIGNATURE-----"

			if line[0..0] == ' ' and lastkey
				e[lastkey] = [] unless e[lastkey]
				e[lastkey] << line[1..-1]
			else
				key, value = line.split(/: */, 2)
				if e[key]
					$logger.warn("[indexrun ##{@mirrorrun_id}] dsc file #{path}/#{name} has duplicate key #{key}")
				elsif value != ""
					e[key] = value
				end
				lastkey = key
			end
		end

		unless hash_has_all_keys(e, %w(Source Version Files))
			$logger.warn("[indexrun ##{@mirrorrun_id}] dsc file #{path}/#{name} has incomplete source information.    [existing keys: #{e.keys.join(', ')}]")
			return
		end

		e['Source-Version'] = e['Version']
		srcpkg = add_srcpkg(e['Source'], e['Source-Version'])
		insert_src_file_from_digest(srcpkg, dscdigest)
		$logger.debug("[indexrun ##{@mirrorrun_id}] Inserting for source #{path}/#{name} (#{dscdigest}): #{e['Source']} #{e['Source-Version']}")

		if e['Checksums-Sha1']
			e['Checksums-Sha1'].each do |line|
				hash, size, itemname, rest = line.split(/ /)
				if $storage.exists?(hash)
					insert_src_file_from_digest(srcpkg, hash)
					$logger.debug("[indexrun ##{@mirrorrun_id}] Inserting for source by digest: #{e['Source']} #{e['Source-Version']}: #{itemname} (#{hash})")
				else
					$logger.warn("[indexrun ##{@mirrorrun_id}] Failed to insert for source by digest, but no file with that digest exists: #{e['Source']} #{e['Source-Version']}: #{itemname} (#{hash})")
				end
			end
		else
			e['Files'].each do |line|
				expect_md5, size, itemname, rest = line.split(/ /)

				candidates = []
				sha1 = get_file_digest(path+"/"+itemname)
				if sha1
					candidates = [sha1]
				else
					@db.query("SELECT DISTINCT hash FROM file JOIN node_with_ts ON file.node_id = node_with_ts.node_id
					           WHERE first_run <= ?
					             AND last_run  >= ?
					             AND node_with_ts.archive_id = ?
					             AND file.name=?", @mirrorrun_run, @mirrorrun_run, @mirrorrun_archive_id, itemname) do |row|
						candidates << row['hash']
					end
				end

				if candidates.length == 0
					$logger.warn("[indexrun ##{@mirrorrun_id}] dsc file #{path}/#{name} references #{itemname} which we do not have")
					next
				elsif candidates.length > 1
					$logger.warn("[indexrun ##{@mirrorrun_id}] dsc file #{path}/#{name} references #{itemname} of which we have different ones!")
				end

				candidates.each do |sha1|
					is_md5 = Digest::MD5.hexdigest( $storage.open(sha1).read )
					if is_md5 != expect_md5
						$logger.warn("[indexrun ##{@mirrorrun_id}] dsc file #{path}/#{name} references #{itemname} with md5 #{expect_md5}) but we have md5 hash #{is_md5}")
						next
					end

					insert_src_file_from_digest(srcpkg, sha1)
					$logger.debug("[indexrun ##{@mirrorrun_id}] Inserting for source #{path}/#{itemname} (#{sha1}): #{e['Source']} #{e['Source-Version']}")
				end
			end
		end
	end

	# If there is no /indices/package-file.map.bz2 we have to fall back to recursing over the tree
	def index_mirrorrun_from_parsing()
		@sourcepkgs = {}
		@binarypkgs = {}
		if not @quick
			query = "SELECT path, name, hash FROM dirtree(?) WHERE filetype='-' AND name SIMILAR TO '%.(deb|udeb|dsc)' AND size != 0"
			args = [@mirrorrun_id]
		else
			query = "SELECT first, path, name, hash FROM dirtree(?) WHERE filetype='-' AND name SIMILAR TO '%.(deb|udeb|dsc)' AND first=? AND size != 0"
			args = [@mirrorrun_id, @mirrorrun_id]
		end
		@db.query(query, *args) do |row|
			case row['name']
				when /\.deb$|\.udeb$/
					index_binary_package(row['path'], row['name'], row['hash'])
				when /\.dsc/
					index_source_package(row['path'], row['name'], row['hash'])
			end
		end

		source = 'recurse'
		source += '(Q)' if @quick
		return source
	end

	def index_mirrorrun(mirrorrun_id, run = nil, archive_id = nil)
		# set @mirrorrun_{id,run,archive_id} class instance "globals"
		if run.nil? or archive_id.nil?
			archive_id, run = _get_archive_and_run_from_mirrorrun(mirrorrun_id)
		end
		@mirrorrun_id = mirrorrun_id;
		@mirrorrun_run = run
		@mirrorrun_archive_id = archive_id

		# and run the indexer - first try from package-map, fall back to dirtree and manual guessing.
		source = index_mirrorrun_from_index()
		return source if source
		source = index_mirrorrun_from_parsing()
		return source
	end

	def index
		if @only_this_mirrorrun
				@db.begin
				row = @db.query_row("SELECT run, (SELECT name FROM archive WHERE archive.archive_id = mirrorrun.archive_id) AS archive
									 FROM mirrorrun WHERE mirrorrun_id = ?", @only_this_mirrorrun)
				if row.nil?
					barf("Mirrorrun ##{@only_this_mirrorrun} does not exist.")
				end
				$logger.info("Indexing mirrorrun ##{@only_this_mirrorrun} of #{row['archive']} from #{row['run']} as requested")
				source = index_mirrorrun(@only_this_mirrorrun)
				@db.dbdo('DELETE FROM indexed_mirrorrun WHERE mirrorrun_id=?', @only_this_mirrorrun)
				@db.insert('indexed_mirrorrun', {'mirrorrun_id' => @only_this_mirrorrun, 'source' => source })
				@db.commit

		else
			@db.query("SELECT mirrorrun_id, run, (SELECT name FROM archive WHERE archive.archive_id = mirrorrun.archive_id) AS archive
					   FROM mirrorrun WHERE NOT mirrorrun_id IN (SELECT mirrorrun_id FROM indexed_mirrorrun) ORDER BY run") do |row|
				@db.begin
				$logger.info("Indexing mirrorrun ##{row['mirrorrun_id']} of #{row['archive']} from #{row['run']}")
				source = index_mirrorrun(row['mirrorrun_id'])
				@db.insert('indexed_mirrorrun', {'mirrorrun_id' => row['mirrorrun_id'], 'source' => source })
				@db.commit
			end
		end
		$logger.info("Indexing done")
	end
end

class SnapshotDumper
	def initialize(db, mirrorrun_id, file=STDOUT)
		@db = db
		@mirrorrun_id = mirrorrun_id
		@file = file
	end

	def dump
		row = @db.query_row("SELECT
				(SELECT name FROM archive WHERE archive.archive_id=mirrorrun.archive_id) AS name,
				run,
				mirrorrun_uuid,
				importing_host
			FROM mirrorrun WHERE mirrorrun_id=?", @mirrorrun_id);
		@file.print "Dump-Format: snapshot.debian.org 0.1\n"
		@file.print "Archive: #{row['name']}\n"
		@file.print "Date: #{row['run']}\n"
		@file.print "UUID: #{row['mirrorrun_uuid']}\n"
		@file.print "ImportingHost: #{row['importing_host']}\n"
		@file.print "Contents:\n"
		@db.query("SELECT filetype, path, name, size, hash, target FROM dirtree(?) ORDER BY path, name NULLS first", @mirrorrun_id) do |row|
			case row['filetype']
				when 'd'
					@file.print " #{row['filetype']} #{row['path']}\n"
				when '-'
					@file.print " #{row['filetype']} #{row['path'] == '/' ? '' : row['path']}/#{row['name']} #{row['size']} #{row['hash']}\n"
				when 'l'
					@file.print " #{row['filetype']} #{row['path'] == '/' ? '' : row['path']}/#{row['name']} #{row['target']}\n"
				else
					throw "Unknown file type #{row['filetype']}"
			end
		end
	end
end

class SnapshotDumpAll
	def initialize(db, path)
		@db = db
		@path = path
	end

	def dump
		parentdir = File.dirname(@path)
		barf("Parent directory #{parentdir} for #{@path} does not exist or is not a directory") unless File.directory?(parentdir)
		unless File.directory?(@path)
			$logger.debug("mkdir #{@path}")
			Dir.mkdir(@path)
		end

		@db.query("SELECT mirrorrun_id FROM mirrorrun ORDER BY mirrorrun_id") do |row|
			target = @path+"/"+row['mirrorrun_id'].to_s
			next if File.exists?(target)
			tmptarget = target + ".tmp."+randstring()+"."+(Process.pid.to_s)

			$logger.debug("dumping mirrorrun ##{row['mirrorrun_id']}")

			begin
				f = File.new(tmptarget,  "w")
				sd = SnapshotDumper.new(@db, row['mirrorrun_id'], f)
				sd.dump
				f.close

				begin
					File.link(tmptarget, target)
				rescue Errno::EEXIST
					# it may have jumped into existance, that's no problem.
				end
			ensure
				File.unlink(tmptarget)
			end
		end
	end
end

class Snapshot
	attr_reader :db

	def initialize(conf)
		@conf = conf
		@db = SnapshotDB.new(conf['db'], $logger)
	end

	def list_archives()
		@db.query("SELECT name FROM archive ORDER BY name") { |row|
			puts row['name']
		}
	end

	def add_archive(options)
		barf("No archive name given") unless options['archive']
		@db.begin
		@db.insert('archive', {'name' => options['archive']})
		@db.commit
		$logger.info("Added new archive #{options['archive']}.")
	end

	def fs_list(options)
		barf("No path name given") unless options['path']
		fs = FSReader.new(options['path'], true)
		fs.each_node() { |p| puts p }
	end

	def import(options)
		barf("No path name given") unless options['path']
		barf("No archive name given") unless options['archive']
		fs = SnapshotImporter.new(@db, options['timetravel'])
		fs.import_from_filesystem(options['path'], options['archive'], options['date'], options['quick'])
	end

	def import_dump(options)
		barf("No path name given") unless options['path']
		fs = SnapshotImporter.new(@db, options['timetravel'])
		fs.import_from_dump(options['path'], options['quick'])
	end

	def index(options)
		pi = PackageIndexer.new(@db, options['quick'], options['mirrorrun_id'])
		pi.index
	end

	def dump(options)
		barf("No mirrorrun given") unless options['mirrorrun_id']
		sd = SnapshotDumper.new(@db, options['mirrorrun_id'])
		sd.dump
	end

	def dumpall(options)
		barf("No dump key in config") unless @conf['dump']
		barf("No dump/dumppath key in config") unless @conf['dump']['dumppath']
		sd = SnapshotDumpAll.new(@db, @conf['dump']['dumppath'])
		sd.dump
	end
end

def makeLogger(conf, options)
	if options['log-to-stderr']
		logger = Logger.new(STDOUT)
	else
		barf("Configuration does not have a log->filename.") unless conf and conf['filename']
		logger = Logger.new(conf['filename'], 7, shift_size = 25 * 1024 * 1024)
	end

	if options['quiet']
		logger.level = Logger::WARN
	elsif options['verbose']
		logger.level = Logger::DEBUG
	else
		logger.level = Logger::INFO
	end

	logger.datetime_format = "%Y-%m-%d %H:%M:%S"

	return logger
end

def loadConfig(file)
	begin
		config = YAML::load( File.open( file ) )
	rescue
		barf("Cannot load config: #{$!}.")
	end
end


def show_help(parser, code=0, io=STDOUT)
	program_name = File.basename($0, '.*')
	io.puts "Usage: #{program_name} [options] <action>"
	io.puts "  Where action is one of the following:"
	io.puts "    list-archives                    List all known archives"
	io.puts "    add-archive                      Add a new archive"
	io.puts "    fs-list                          List a directory recursively"
	io.puts "    import                           Import a directory"
	io.puts "    import-dump                      Import a dump"
	io.puts "    index                            Index packages in archives"
	io.puts "    dump                             Dump a mirrorrun's metadata to stdout"
	io.puts "    dumpall                          Dump all mirrorruns' metadata to disk"
	io.puts "  Options:"
	io.puts parser.summarize
	exit(code)
end

options = {}
ARGV.options do |opts|
	opts.on_tail("-c", "--config=<f>"   , String, "Config file")                                    {  |x| options['config'] = x }
	opts.on_tail("-s", "--log-to-stderr", nil,    "Log to stderr instead of the configure logfile") {  |x| options['log-to-stderr'] = x }
	opts.on_tail("-q", "--quiet"        , nil,    "Do not show info level notices")                 {  |x| options['quiet'] = x }
	opts.on_tail("-v", "--verbose"      , nil,    "Be more verbose")                                {  |x| options['verbose'] = x }
	opts.on_tail("-a", "--archive=<a>"  , String, "Archive to add/operate on")                      {  |x| options['archive'] = x }
	opts.on_tail("-p", "--path=<p>"     , String, "Path to recurse/import")                         {  |x| options['path'] = x }
	opts.on_tail("-d", "--date=<d>"     , String, "Date associated with an import (if not 'now')")  {  |x| options['date'] = x }
	opts.on_tail("-Q", "--quick"        , nil,    "Quick import (use [cm]time and filesize, not hash), Quick index (only index files that were added in a mirrorrun)")  {  |x| options['quick'] = x }
	opts.on_tail("-T", "--timetravel"   , nil,    "Allow imports at dates that are not later than all existing mirrorruns")  {  |x| options['timetravel'] = x }
	opts.on_tail("-m", "--mirrorrun=<id>" , Integer, "Operate on a particular mirrorrun (index/dump)")  {  |x| options['mirrorrun_id'] = x }

	opts.on_tail("-h", "--help", "Display this help screen")        { show_help(opts) }
	opts.parse!
end


File.umask(0022)


show_help(ARGV.options, 1, STDERR) unless options['config']
show_help(ARGV.options, 1, STDERR) unless ARGV.length == 1
action = ARGV.shift

config = loadConfig(options['config'])
$logger = makeLogger(config['log'], options)

barf("Configuration does not have a snapshot->farmpath.") unless config and config['snapshot'] and config['snapshot']['farmpath']
barf("Please run with TZ=UTC") unless Time.now.zone == "UTC"
snapshot = Snapshot.new(config)
$storage = FileBackend.new(config['snapshot']['farmpath'], snapshot.db)

case action
	when "list-archives"
		snapshot.list_archives
	when "add-archive"
		snapshot.add_archive(options)
	when "fs-list"
		snapshot.fs_list(options)
	when "import"
		snapshot.import(options)
	when "import-dump"
		snapshot.import_dump(options)
	when 'index'
		require 'bzip2'
		snapshot.index(options)
	when "dump"
		snapshot.dump(options)
	when "dumpall"
		snapshot.dumpall(options)
	else
		barf("Unknown action #{action}.")
end

# vim:set ts=4:
# vim:set shiftwidth=4:
